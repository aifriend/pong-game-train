<div align="center">

# ğŸ“ Pong RL Training Environment

### Train AI agents to master the classic game of Pong using Deep Reinforcement Learning

[![Python Version](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c.svg)](https://pytorch.org/)
[![Gymnasium](https://img.shields.io/badge/Gymnasium-0.29%2B-00ADD8.svg)](https://gymnasium.farama.org/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation) â€¢ [Examples](#-usage-examples) â€¢ [Contributing](#-contributing)

</div>

---

## ğŸ“– About

A production-ready reinforcement learning environment for training AI agents to play Pong. This project provides both **pygame-based** and **pure Python headless** environments, custom **Double DQN** implementation, and integration with **Stable-Baselines3** for rapid experimentation.

## ğŸ¥ Demo

> **Note:** Add screenshots or GIFs here to showcase your trained agents in action!
> 
> ```bash
> # Record a demo of your trained agent
> PYTHONPATH=. python scripts/play_against_agent.py checkpoints/checkpoint_episode_1000.pth
> ```

**Training Progress Visualization:**

![Training Progress](training_progress.png)

*Example training progress showing episode rewards, win rates, and Q-values over time.*

---

## ğŸŒŸ Why This Project?

This isn't just another Pong implementationâ€”it's a **complete RL training framework** designed for:

| Feature | Benefit |
|---------|---------|
| ğŸš€ **Headless Mode** | Train 10x faster without graphics overhead |
| ğŸ§  **Multiple Algorithms** | Compare DQN, Double DQN, PPO side-by-side |
| ğŸ“Š **Production-Ready** | TensorBoard, checkpoints, logging out of the box |
| ğŸ“ **Educational** | Comprehensive docs, tutorials, and clean code |
| ğŸ”§ **Customizable** | Easy to modify rewards, observations, and network architecture |
| ğŸƒ **Fast Iteration** | Quick setup, fast training, immediate testing |

Perfect for:
- ğŸ“ **Students** learning reinforcement learning
- ğŸ”¬ **Researchers** prototyping new RL algorithms
- ğŸ’¼ **Engineers** building production RL systems
- ğŸ® **Hobbyists** interested in game AI

---

## âœ¨ Features

### ğŸš€ Dual Environment Support
- **PongEnv**: Full pygame-based environment with graphics and audio
- **PongHeadlessEnv**: Pure Python environment for ultra-fast headless training (no display needed)
- **Self-play training**: Agent plays against itself for improved learning

### ğŸ§  Advanced RL Algorithms
- **Double DQN**: Custom PyTorch implementation with target networks and experience replay
- **Stable-Baselines3**: PPO and DQN with optimized hyperparameters
- **TensorBoard integration**: Real-time training monitoring and visualization

### ğŸ”§ Training Features
- **Optimized hyperparameters**: Fast learning with proper exploration/exploitation balance
- **Normalized observations**: All inputs scaled to [0,1] or [-1,1] for stable neural network training
- **Intelligent reward shaping**: Encourages ball hitting and strategic positioning
- **Efficient memory buffer**: Numpy-based experience replay with O(1) sampling
- **Checkpoint system**: Save and test models during training

### ğŸ® Interactive Testing
- **Human vs Agent**: Play against trained models to test their skill
- **Progressive testing**: Test different checkpoints to see learning progress
- **Multiple game modes**: Configure ball speed, max score, and episode length

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+** (3.10 recommended)
- **pip** package manager
- **Git**
- Optional: **CUDA** for GPU acceleration

### Installation

#### Linux / macOS

```bash
# Clone the repository
git clone https://github.com/your-username/pong-rl-env.git
cd pong-rl-env

# Create and activate virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Verify installation
python -c "import pong; import gymnasium; import torch; print('âœ“ All dependencies installed!')"
```

#### Windows

```bash
# Clone the repository
git clone https://github.com/your-username/pong-rl-env.git
cd pong-rl-env

# Create and activate virtual environment
python -m venv .venv
.venv\Scripts\activate

# Install dependencies
pip install --upgrade pip
pip install -r requirements.txt

# Verify installation
python -c "import pong; import gymnasium; import torch; print('âœ“ All dependencies installed!')"
```

#### Optional: GPU Support

For NVIDIA GPU acceleration (10x faster training):

```bash
# Install PyTorch with CUDA support
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### ğŸ¯ Quick Reference Card

| Task | Command |
|------|---------|
| **Train Agent (Fast)** | `PYTHONPATH=. python trainer/main.py` |
| **Train with SB3** | `PYTHONPATH=. python scripts/train_pong_agent.py` |
| **Play vs Agent** | `PYTHONPATH=. python scripts/play_against_agent.py checkpoints/checkpoint_episode_1000.pth` |
| **Test Environment** | `PYTHONPATH=. python tests/test_gym_integration.py` |
| **Monitor Training** | `tensorboard --logdir ./tensorboard_dqn/` |
| **Manual Play** | `PYTHONPATH=. python scripts/play.py` |

### Train Your First Agent (Step-by-Step)

**Option 1: Custom Double DQN (Recommended for beginners)**

```bash
# Start training with headless mode (fast)
PYTHONPATH=. python trainer/main.py

# Training will:
# - Save checkpoints every 100 episodes to checkpoints/
# - Log metrics to TensorBoard in tensorboard_dqn/
# - Save final weights to final_weights.pth
# - Display training stats in terminal

# Typical training time: 1-2 hours for 1000 episodes
# Expected win rate after 1000 episodes: ~70%
```

**Option 2: Stable-Baselines3 PPO/DQN**

```bash
# Train using production-grade algorithms
PYTHONPATH=. python scripts/train_pong_agent.py

# Benefits:
# - Automatic hyperparameter optimization
# - Vectorized environments (4x parallel)
# - Built-in evaluation callbacks
```

### Play Against Your Trained Agent

```bash
# Test a specific checkpoint
PYTHONPATH=. python scripts/play_against_agent.py checkpoints/checkpoint_episode_1000.pth

# Controls:
# - W: Move paddle up
# - S: Move paddle down
# - ESC: Quit

# Challenge: Can you beat your own AI? ğŸ†
```

## ğŸ“Š Monitor Training

### TensorBoard
```bash
# Start TensorBoard (in another terminal)
tensorboard --logdir ./tensorboard_dqn/

# Open http://localhost:6006 in browser
```

### Real-time Stats
- Episode scores and win rates
- Epsilon decay and learning progress  
- Network loss and Q-values
- Training speed (episodes/sec)

## ğŸ¯ Training Options

### 1. Custom Double DQN (Recommended)
```bash
PYTHONPATH=. python trainer/main.py
```

**Features:**
- Self-play training (agent vs agent)
- Normalized observations for stable learning
- Double DQN to reduce overestimation bias
- TensorBoard logging
- Checkpoints every 100 episodes

### 2. Stable-Baselines3 PPO/DQN
```bash
PYTHONPATH=. python scripts/train_pong_agent.py
```

**Features:**
- Choose between PPO or DQN algorithms
- Vectorized training (4 parallel environments)
- Evaluation callbacks
- Automatic best model saving

## ğŸ® Environment Details

### Observation Space (9-dimensional, normalized)
| Index | Feature | Range | Description |
|-------|---------|-------|-------------|
| 0 | Ball X | [0, 1] | Horizontal ball position |
| 1 | Ball Y | [0, 1] | Vertical ball position |
| 2 | Ball VX | [-1, 1] | Horizontal ball velocity |
| 3 | Ball VY | [-1, 1] | Vertical ball velocity |
| 4 | Player Y | [0, 1] | Player paddle position |
| 5 | Opponent Y | [0, 1] | Opponent paddle position |
| 6 | Ball Distance | [0, 1] | Distance from ball to player |
| 7 | Player Score | [0, 1] | Player score (normalized) |
| 8 | Opponent Score | [0, 1] | Opponent score (normalized) |

### Action Space
| Action | Description |
|--------|-------------|
| 0 | Stay/No movement |
| 1 | Move paddle up |
| 2 | Move paddle down |

### Reward System
| Event | Reward | Purpose |
|-------|--------|---------|
| Score point | +1.0 | Primary objective |
| Opponent scores | -1.0 | Penalty for failure |
| Hit ball | +0.1 | Encourage ball interaction |
| Good positioning | +0.001 | Subtle positioning guidance |
| Survival | +0.0001 | Encourage longer rallies |

## ğŸ“ Project Structure

```
Pong/
â”œâ”€â”€ pong/                     # Core game package
â”‚   â”œâ”€â”€ env/                  # Gym environments
â”‚   â”‚   â”œâ”€â”€ pong_gym_env.py   # Pygame-based environment
â”‚   â”‚   â””â”€â”€ pong_headless.py  # Pure Python environment
â”‚   â”œâ”€â”€ game/                 # Game components
â”‚   â”‚   â”œâ”€â”€ ball.py          # Ball physics
â”‚   â”‚   â”œâ”€â”€ player.py        # Player paddle
â”‚   â”‚   â”œâ”€â”€ opponent.py      # AI opponent
â”‚   â”‚   â””â”€â”€ game_manager.py  # Game state management
â”‚   â””â”€â”€ constants.py         # Game constants
â”œâ”€â”€ trainer/                  # Custom DQN implementation
â”‚   â”œâ”€â”€ main.py              # Training entry point
â”‚   â”œâ”€â”€ the_agent_pytorch.py # Double DQN agent
â”‚   â”œâ”€â”€ agent_memory.py      # Experience replay buffer
â”‚   â”œâ”€â”€ environment.py       # Training environment wrapper
â”‚   â””â”€â”€ config.py           # Configuration management
â”œâ”€â”€ scripts/                  # Training scripts
â”‚   â”œâ”€â”€ train_pong_agent.py  # Stable-Baselines3 training
â”‚   â”œâ”€â”€ play_against_agent.py# Human vs agent testing
â”‚   â””â”€â”€ play.py             # Manual game play
â”œâ”€â”€ tests/                   # Test suite
â”‚   â””â”€â”€ test_gym_integration.py
â”œâ”€â”€ learn/                   # Documentation & tutorials
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ environment_guide.md
â”‚   â””â”€â”€ stable_baselines3_tutorial.md
â”œâ”€â”€ docs/                    # Additional documentation
â”‚   â””â”€â”€ README_GYM.md
â””â”€â”€ resources/              # Game assets
    â”œâ”€â”€ ball.png
    â”œâ”€â”€ paddle.png
    â””â”€â”€ *.ogg               # Sound files
```

## ğŸ® Usage Examples

### Basic Environment Testing
```python
import gymnasium as gym
from pong import register_headless_env

# Test headless environment
register_headless_env()
env = gym.make('PongHeadless-v0', render_mode=None)

obs, info = env.reset()
for _ in range(1000):
    action = env.action_space.sample()
    obs, reward, done, truncated, info = env.step(action)
    if done or truncated:
        obs, info = env.reset()
env.close()
```

### Train Custom DQN Agent
```python
from trainer import Agent, environment

# Create agent
agent = Agent(
    possible_actions=[0, 1, 2],
    starting_mem_len=5000,
    max_mem_len=100000,
    learn_rate=0.0005,
    observation_dim=9,
    tensorboard_log='./logs/',
    learn_every=4
)

# Create self-play environment
env = environment.make_env('PongHeadless-v0', render_mode=None, self_play=True)

# Train
for episode in range(1000):
    score = environment.play_episode(env, agent)
    agent.log_episode(episode, score, agent.total_timesteps)
```

### Load and Test Agent
```python
# Load trained agent
agent.load_weights('checkpoints/checkpoint_episode_1000.pth')

# Test performance
env = gym.make('PongHeadless-v0', render_mode='human')
obs, _ = env.reset()

while True:
    action = agent.get_action(obs)
    obs, reward, done, truncated, info = env.step(action)
    if done or truncated:
        break
```

## âš¡ Performance Benchmarks

### Training Speed Comparison

| Environment | Steps/Second | Episodes/Min | Use Case |
|-------------|--------------|--------------|----------|
| **PongHeadlessEnv** (no render) | ~528 | ~12 | âš¡ Production training |
| **PongEnv** (headless) | ~300 | ~7 | ğŸ”§ Development |
| **PongEnv** (with display) | ~50-100 | ~1-2 | ğŸ‘€ Visual debugging |

*Benchmarked on M1 MacBook Pro (2021), 16GB RAM*

### Learning Efficiency Improvements

| Optimization | Speedup | Impact |
|--------------|---------|--------|
| ğŸ“Š **Observation normalization** | 2-3x | Faster convergence to optimal policy |
| ğŸ¯ **Double DQN** | 1.5x | Reduced Q-value overestimation |
| ğŸ’¾ **Optimized memory buffer** | 10x | 5K vs 50K starting experiences |
| ğŸ¤– **Self-play training** | 2x | Continuous difficulty scaling |

### Typical Training Results

```
Episodes 1-100:    Average Score: -5.2  (random play)
Episodes 100-500:  Average Score: -2.1  (learning basics)
Episodes 500-1000: Average Score:  0.3  (competent play)
Episodes 1000+:    Average Score:  3.7  (strong performance)
```

## ğŸ§ª Testing

```bash
# Test headless environment
PYTHONPATH=. python pong/env/pong_headless.py

# Test pygame environment (requires display)
PYTHONPATH=. python tests/test_gym_integration.py

# Test training pipeline
PYTHONPATH=. python -c "
from trainer import Agent, environment
agent = Agent([0,1,2], 100, 1000, 1.0, 0.001, 9)
env = environment.make_env('Pong-v0', render_mode=None, self_play=True)
score = environment.play_episode(env, agent)
print(f'Episode score: {score:.2f}')
"
```

## ğŸ”§ Configuration

### Training Configurations
Three pre-configured training modes available in `trainer/config.py`:

- **DEFAULT_CONFIG**: Balanced training
- **FAST_TRAINING_CONFIG**: Quick experimentation
- **DEBUG_CONFIG**: Development and debugging
- **PRODUCTION_CONFIG**: Long-term training

### Environment Options
```python
# Headless training (fast)
env = gym.make('PongHeadless-v0', 
               render_mode=None, 
               agent_controlled_opponent=True,
               max_score=11,
               max_steps=10000)

# Visual training (slower)
env = gym.make('Pong-v0',
               render_mode='human',
               agent_controlled_opponent=True,
               max_score=5,
               max_steps=5000)
```

## ğŸ“ˆ Training Tips

### For Best Results
1. **Start with headless**: Use `PongHeadlessEnv` for initial training
2. **Monitor with TensorBoard**: Track loss, epsilon, and episode rewards
3. **Test checkpoints**: Use `play_against_agent.py` to evaluate progress
4. **Adjust hyperparameters**: Use `trainer/config.py` for experiments

### Common Issues & Solutions

| Issue | Solution |
|-------|----------|
| **Slow training** | Use `render_mode=None` and `PongHeadlessEnv` for 10x speed boost |
| **Poor convergence** | Verify observation normalization is enabled and reward shaping is balanced |
| **Memory errors** | Reduce `max_mem_len` in config or use smaller batch sizes |
| **pygame.error** | Install pygame: `pip install pygame==2.1.2` |
| **CUDA out of memory** | Reduce batch size in `trainer/config.py` or use CPU |
| **No module named 'pong'** | Set PYTHONPATH: `export PYTHONPATH=.` or use `PYTHONPATH=. python ...` |
| **Display issues on headless server** | Use `PongHeadlessEnv` which doesn't require a display |
| **TensorBoard not loading** | Check logs directory: `tensorboard --logdir ./tensorboard_dqn/` |

## ğŸ† Advanced Features

### Self-Play Training
- Agent trains by playing against itself
- Opponent uses reduced epsilon for consistent challenge
- Both sides contribute to learning experience

### Observation Normalization
- All features scaled to [0,1] or [-1,1] range
- Dramatically improves neural network training stability
- Consistent feature importance across different scales

### Double DQN Algorithm
- Reduces Q-value overestimation bias
- Uses main network for action selection
- Uses target network for action evaluation
- More stable learning than vanilla DQN

### Efficient Memory Buffer
- Numpy-based circular buffer
- O(1) insertion and batch sampling
- Proper handling of episode boundaries
- Optional prioritized experience replay

## ğŸ¯ Next Steps

### Experiment Ideas
- Try different network architectures in `the_agent_pytorch.py`
- Implement prioritized experience replay
- Add curriculum learning with varying difficulty
- Experiment with different reward functions
- Add multi-agent tournament training

### Performance Optimization
- Use GPU training: ensure CUDA PyTorch installation
- Implement frame stacking for visual learning
- Add action repeat for faster learning
- Experiment with different optimizers (RMSprop, AdamW)

## ğŸ“š Documentation

- `learn/`: Comprehensive guides and tutorials
- `docs/`: API documentation and environment specs
- `trainer/README.md`: Custom DQN implementation details

## â“ FAQ

<details>
<summary><b>Can I train on a machine without a display?</b></summary>

Yes! Use `PongHeadlessEnv` which is specifically designed for headless training:
```python
env = gym.make('PongHeadless-v0', render_mode=None)
```
</details>

<details>
<summary><b>How long does training take?</b></summary>

- **Quick test**: 100 episodes (~5-10 minutes)
- **Basic competence**: 1,000 episodes (~1-2 hours)
- **Strong performance**: 5,000+ episodes (4-8 hours)
- **Near-optimal**: 10,000+ episodes (8-16 hours)

Speed depends heavily on your hardware and whether you use headless mode.
</details>

<details>
<summary><b>Can I use GPU acceleration?</b></summary>

Yes! Install PyTorch with CUDA support:
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu118
```
The agent will automatically use CUDA if available.
</details>

<details>
<summary><b>How do I continue training from a checkpoint?</b></summary>

```python
agent.load_weights('checkpoints/checkpoint_episode_1000.pth')
# Continue training...
```
</details>

<details>
<summary><b>Can I customize the reward function?</b></summary>

Yes! Edit the reward calculation in `pong/env/pong_gym_env.py` or `pong/env/pong_headless.py` in the `step()` method.
</details>

---

## ğŸ¤ Contributing

We welcome contributions! Here's how to get started:

### Development Setup

```bash
# Clone the repository
git clone https://github.com/your-username/pong-rl-env.git
cd pong-rl-env

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install in development mode with dev dependencies
pip install -e ".[dev]"
```

### Contribution Guidelines

1. **Fork** the repository
2. **Create** a feature branch (`git checkout -b feature/amazing-feature`)
3. **Write** tests for new features
4. **Run** tests: `python -m pytest tests/`
5. **Format** code: `black . && flake8`
6. **Commit** changes (`git commit -m 'Add amazing feature'`)
7. **Push** to branch (`git push origin feature/amazing-feature`)
8. **Open** a Pull Request

### Areas for Contribution

- ğŸ› Bug fixes and improvements
- ğŸ“š Documentation enhancements
- âœ¨ New RL algorithms (A3C, SAC, etc.)
- ğŸ® Additional game modes or environments
- ğŸ“Š Visualization and analysis tools
- ğŸ§ª Additional tests and benchmarks

---

## ğŸ“„ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2025 Pong RL Environment Contributors

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸ“š Citation

If you use this project in your research or educational work, please cite:

```bibtex
@software{pong_rl_env_2025,
  title = {Pong RL Training Environment: A Comprehensive Framework for Reinforcement Learning},
  author = {Your Name},
  year = {2025},
  url = {https://github.com/your-username/pong-rl-env},
  version = {1.0.0}
}
```

---

## ğŸ™ Acknowledgments

This project builds upon excellent open-source libraries and research:

- **[Gymnasium](https://gymnasium.farama.org/)** - Modern RL environment interface
- **[Stable-Baselines3](https://stable-baselines3.readthedocs.io/)** - High-quality RL implementations  
- **[PyTorch](https://pytorch.org/)** - Deep learning framework
- **[Pygame](https://www.pygame.org/)** - Game development library
- The RL research community for continuous innovation

Special thanks to:
- Mnih et al. for the original DQN paper (2015)
- Van Hasselt et al. for Double DQN (2016)
- Schulman et al. for PPO algorithm (2017)

---

## ğŸ“ Support

- ğŸ“– **Documentation**: Check the `docs/` and `learn/` directories
- ğŸ› **Issues**: [GitHub Issues](https://github.com/your-username/pong-rl-env/issues)
- ğŸ’¬ **Discussions**: [GitHub Discussions](https://github.com/your-username/pong-rl-env/discussions)
- â­ **Star** this repo if you find it useful!

---

## ğŸ—ºï¸ Roadmap

- [ ] Multi-agent tournament mode
- [ ] Prioritized Experience Replay (PER)
- [ ] Curriculum learning with difficulty progression
- [ ] Visual observation mode (pixel-based)
- [ ] Web-based training dashboard
- [ ] Pre-trained model zoo
- [ ] Integration with Ray RLlib
- [ ] Docker containerization

---

<div align="center">

### â­ Star this repository if you found it helpful!

**Happy Training! ğŸ®ğŸ¤–**

*Train smart, play hard, and may your agents become Pong masters!*

Made with â¤ï¸ by the RL community

[Back to Top](#-pong-rl-training-environment)

</div>
